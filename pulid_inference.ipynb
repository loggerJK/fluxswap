{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2484ba26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvlab17/anaconda3/envs/faceswap_omini/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please 'pip install apex'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eda606b3a7d4009b9217bf4cbc1907e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvlab17/anaconda3/envs/faceswap_omini/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/cvlab17/anaconda3/envs/faceswap_omini/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ba1e96402e4b6bb9800685d040371b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\n",
      "find model: ./models/antelopev2/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\n",
      "find model: ./models/antelopev2/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\n",
      "find model: ./models/antelopev2/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\n",
      "find model: ./models/antelopev2/glintr100.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\n",
      "find model: ./models/antelopev2/scrfd_10g_bnkps.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "set det-size: (640, 640)\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\n",
      "loading from pulid_ca\n",
      "loading from pulid_encoder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fae859abd71468fb48925af149fc070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of FluxTransformer2DModelCA were not initialized from the model checkpoint at black-forest-labs/FLUX.1-Krea-dev and are newly initialized: ['pulid_ca.19.norm1.weight', 'clip_vision_model.blocks.1.mlp.ffn_ln.bias', 'clip_vision_model.blocks.1.mlp.w1.weight', 'clip_vision_model.blocks.15.attn.k_proj.weight', 'clip_vision_model.blocks.10.norm2.bias', 'clip_vision_model.blocks.5.mlp.w3.weight', 'clip_vision_model.blocks.14.mlp.w2.weight', 'clip_vision_model.blocks.4.attn.rope.freqs_sin', 'pulid_ca.12.to_kv.weight', 'pulid_encoder.layers.7.0.to_kv.weight', 'clip_vision_model.blocks.19.attn.rope.freqs_cos', 'clip_vision_model.blocks.23.attn.proj.weight', 'clip_vision_model.blocks.21.mlp.ffn_ln.weight', 'clip_vision_model.blocks.18.norm2.weight', 'clip_vision_model.blocks.7.attn.inner_attn_ln.weight', 'pulid_encoder.layers.0.0.to_out.weight', 'pulid_ca.1.norm2.weight', 'clip_vision_model.blocks.6.norm2.weight', 'clip_vision_model.blocks.11.norm2.weight', 'pulid_encoder.mapping_2.4.weight', 'clip_vision_model.blocks.10.attn.q_bias', 'pulid_ca.17.to_out.weight', 'pulid_encoder.layers.3.0.norm2.weight', 'clip_vision_model.blocks.17.norm2.weight', 'clip_vision_model.blocks.5.mlp.ffn_ln.bias', 'clip_vision_model.blocks.10.norm1.bias', 'pulid_encoder.layers.9.1.0.bias', 'clip_vision_model.blocks.13.attn.q_bias', 'clip_vision_model.blocks.21.mlp.w2.bias', 'clip_vision_model.blocks.8.attn.v_bias', 'clip_vision_model.blocks.3.attn.q_proj.weight', 'pulid_encoder.mapping_1.0.bias', 'pulid_encoder.mapping_1.4.weight', 'clip_vision_model.blocks.4.mlp.w1.weight', 'clip_vision_model.blocks.8.attn.k_proj.weight', 'pulid_ca.19.to_kv.weight', 'clip_vision_model.blocks.11.attn.proj.weight', 'clip_vision_model.blocks.7.attn.proj.bias', 'pulid_encoder.layers.4.0.norm1.weight', 'pulid_encoder.layers.3.0.norm1.bias', 'clip_vision_model.blocks.4.attn.k_proj.weight', 'clip_vision_model.blocks.14.attn.q_proj.weight', 'clip_vision_model.blocks.3.attn.k_proj.weight', 'clip_vision_model.blocks.20.mlp.w3.weight', 'pulid_encoder.layers.6.1.0.weight', 'pulid_encoder.layers.6.1.0.bias', 'clip_vision_model.blocks.23.mlp.ffn_ln.weight', 'pulid_ca.9.to_out.weight', 'pulid_ca.17.norm2.bias', 'clip_vision_model.blocks.4.attn.proj.weight', 'clip_vision_model.blocks.8.attn.inner_attn_ln.weight', 'clip_vision_model.blocks.13.mlp.ffn_ln.weight', 'pulid_encoder.layers.4.1.1.weight', 'pulid_encoder.layers.8.1.3.weight', 'pulid_encoder.mapping_4.6.weight', 'pulid_ca.14.norm1.weight', 'pulid_encoder.layers.8.1.0.bias', 'clip_vision_model.blocks.11.attn.proj.bias', 'pulid_ca.12.to_out.weight', 'clip_vision_model.blocks.8.norm1.bias', 'clip_vision_model.blocks.13.mlp.w2.bias', 'pulid_encoder.mapping_2.1.bias', 'pulid_encoder.proj_out', 'clip_vision_model.blocks.19.mlp.w2.bias', 'pulid_ca.12.norm1.bias', 'clip_vision_model.blocks.21.attn.inner_attn_ln.bias', 'clip_vision_model.blocks.2.mlp.ffn_ln.weight', 'clip_vision_model.blocks.19.attn.q_bias', 'clip_vision_model.blocks.15.norm2.weight', 'clip_vision_model.blocks.3.attn.q_bias', 'clip_vision_model.blocks.16.attn.v_proj.weight', 'clip_vision_model.blocks.17.attn.proj.bias', 'pulid_ca.8.norm1.bias', 'pulid_encoder.layers.5.0.to_out.weight', 'clip_vision_model.blocks.14.attn.v_proj.weight', 'clip_vision_model.blocks.10.attn.proj.bias', 'pulid_encoder.layers.5.1.0.bias', 'pulid_encoder.mapping_4.6.bias', 'clip_vision_model.blocks.3.mlp.ffn_ln.weight', 'clip_vision_model.blocks.8.mlp.w1.weight', 'clip_vision_model.blocks.16.attn.rope.freqs_cos', 'pulid_encoder.mapping_1.6.weight', 'pulid_encoder.layers.3.0.to_kv.weight', 'clip_vision_model.blocks.7.attn.q_proj.weight', 'clip_vision_model.blocks.9.attn.inner_attn_ln.bias', 'pulid_ca.1.norm2.bias', 'clip_vision_model.blocks.9.attn.rope.freqs_sin', 'clip_vision_model.blocks.8.mlp.w3.bias', 'clip_vision_model.blocks.12.attn.v_proj.weight', 'pulid_ca.12.norm2.weight', 'clip_vision_model.blocks.5.attn.q_bias', 'pulid_encoder.layers.7.0.norm1.weight', 'clip_vision_model.blocks.20.norm2.weight', 'pulid_ca.14.norm2.weight', 'pulid_ca.0.norm2.bias', 'pulid_encoder.mapping_1.3.weight', 'clip_vision_model.blocks.10.norm1.weight', 'clip_vision_model.blocks.21.norm2.bias', 'clip_vision_model.blocks.9.attn.k_proj.weight', 'clip_vision_model.blocks.7.mlp.w1.bias', 'clip_vision_model.blocks.12.norm2.weight', 'clip_vision_model.blocks.9.norm1.weight', 'clip_vision_model.blocks.7.norm1.bias', 'clip_vision_model.blocks.20.attn.q_bias', 'clip_vision_model.blocks.8.attn.inner_attn_ln.bias', 'clip_vision_model.blocks.18.mlp.w3.weight', 'pulid_encoder.mapping_1.4.bias', 'clip_vision_model.blocks.12.attn.proj.bias', 'clip_vision_model.blocks.13.attn.rope.freqs_cos', 'pulid_ca.18.to_out.weight', 'clip_vision_model.blocks.11.mlp.w3.bias', 'clip_vision_model.blocks.1.attn.inner_attn_ln.weight', 'clip_vision_model.blocks.21.attn.proj.bias', 'clip_vision_model.blocks.20.mlp.w1.weight', 'pulid_ca.7.to_q.weight', 'clip_vision_model.blocks.16.mlp.w2.bias', 'clip_vision_model.blocks.13.attn.k_proj.weight', 'clip_vision_model.blocks.15.norm1.bias', 'clip_vision_model.blocks.18.mlp.w1.weight', 'clip_vision_model.blocks.18.mlp.w1.bias', 'clip_vision_model.blocks.22.mlp.w2.bias', 'clip_vision_model.blocks.9.mlp.w3.bias', 'pulid_ca.6.to_kv.weight', 'pulid_ca.8.to_q.weight', 'clip_vision_model.blocks.13.attn.q_proj.weight', 'clip_vision_model.blocks.2.mlp.ffn_ln.bias', 'clip_vision_model.blocks.10.mlp.w2.weight', 'clip_vision_model.blocks.17.attn.proj.weight', 'pulid_encoder.layers.5.1.0.weight', 'pulid_encoder.layers.7.0.to_out.weight', 'pulid_ca.15.norm2.bias', 'pulid_ca.5.norm1.bias', 'pulid_encoder.mapping_1.0.weight', 'clip_vision_model.blocks.22.mlp.ffn_ln.weight', 'pulid_ca.10.norm2.weight', 'clip_vision_model.blocks.22.attn.k_proj.weight', 'pulid_encoder.layers.4.0.norm1.bias', 'clip_vision_model.blocks.5.norm1.weight', 'pulid_ca.17.to_q.weight', 'pulid_encoder.mapping_3.6.bias', 'clip_vision_model.blocks.14.norm1.weight', 'clip_vision_model.blocks.3.attn.inner_attn_ln.bias', 'pulid_ca.7.norm1.bias', 'clip_vision_model.blocks.4.norm1.weight', 'clip_vision_model.blocks.16.mlp.ffn_ln.weight', 'clip_vision_model.blocks.17.attn.k_proj.weight', 'clip_vision_model.blocks.12.mlp.w1.bias', 'clip_vision_model.blocks.12.attn.inner_attn_ln.bias', 'pulid_encoder.mapping_0.0.weight', 'clip_vision_model.blocks.18.mlp.w2.bias', 'clip_vision_model.blocks.22.attn.q_proj.weight', 'clip_vision_model.blocks.4.mlp.w3.bias', 'clip_vision_model.blocks.8.norm2.weight', 'clip_vision_model.blocks.2.norm1.bias', 'pulid_encoder.mapping_2.4.bias', 'pulid_encoder.mapping_1.6.bias', 'clip_vision_model.blocks.18.attn.proj.bias', 'pulid_encoder.mapping_2.0.bias', 'clip_vision_model.blocks.22.attn.q_bias', 'pulid_ca.13.norm2.weight', 'pulid_encoder.mapping_4.3.bias', 'clip_vision_model.blocks.6.mlp.ffn_ln.bias', 'pulid_encoder.layers.7.0.norm2.bias', 'clip_vision_model.blocks.18.attn.inner_attn_ln.weight', 'pulid_ca.7.to_kv.weight', 'clip_vision_model.blocks.19.mlp.w3.weight', 'clip_vision_model.blocks.18.attn.rope.freqs_sin', 'pulid_ca.11.norm1.weight', 'clip_vision_model.rope.freqs_sin', 'clip_vision_model.blocks.11.mlp.ffn_ln.bias', 'clip_vision_model.blocks.23.mlp.w1.weight', 'clip_vision_model.blocks.22.norm1.weight', 'clip_vision_model.blocks.15.mlp.w3.bias', 'clip_vision_model.blocks.17.attn.q_proj.weight', 'pulid_encoder.layers.2.1.1.weight', 'clip_vision_model.blocks.0.mlp.w1.bias', 'clip_vision_model.blocks.11.attn.v_bias', 'clip_vision_model.blocks.5.mlp.w2.bias', 'pulid_encoder.layers.3.1.3.weight', 'pulid_ca.5.to_q.weight', 'clip_vision_model.blocks.23.attn.inner_attn_ln.bias', 'clip_vision_model.blocks.20.attn.v_proj.weight', 'pulid_encoder.mapping_3.4.weight', 'clip_vision_model.blocks.10.attn.k_proj.weight', 'clip_vision_model.blocks.12.mlp.w1.weight', 'clip_vision_model.blocks.1.mlp.ffn_ln.weight', 'clip_vision_model.blocks.3.mlp.w1.bias', 'clip_vision_model.blocks.12.attn.q_bias', 'pulid_ca.12.norm1.weight', 'clip_vision_model.blocks.19.attn.rope.freqs_sin', 'clip_vision_model.blocks.0.mlp.ffn_ln.bias', 'clip_vision_model.blocks.23.attn.rope.freqs_sin', 'clip_vision_model.blocks.7.attn.v_proj.weight', 'clip_vision_model.blocks.14.attn.proj.bias', 'clip_vision_model.blocks.17.mlp.w3.weight', 'pulid_ca.7.norm1.weight', 'clip_vision_model.blocks.22.norm2.weight', 'pulid_ca.4.norm1.bias', 'clip_vision_model.blocks.13.mlp.w3.bias', 'clip_vision_model.blocks.2.norm1.weight', 'clip_vision_model.blocks.14.attn.k_proj.weight', 'clip_vision_model.blocks.9.mlp.w2.bias', 'clip_vision_model.blocks.11.mlp.w2.bias', 'pulid_encoder.layers.8.1.0.weight', 'pulid_encoder.layers.7.0.norm2.weight', 'clip_vision_model.blocks.20.norm2.bias', 'clip_vision_model.blocks.20.attn.proj.weight', 'clip_vision_model.patch_embed.proj.weight', 'clip_vision_model.blocks.22.attn.rope.freqs_sin', 'clip_vision_model.blocks.22.attn.v_bias', 'clip_vision_model.blocks.17.mlp.w1.bias', 'clip_vision_model.blocks.7.attn.v_bias', 'pulid_ca.16.norm2.bias', 'clip_vision_model.blocks.11.attn.q_proj.weight', 'clip_vision_model.blocks.19.attn.k_proj.weight', 'clip_vision_model.blocks.23.norm1.weight', 'pulid_encoder.layers.6.0.to_out.weight', 'clip_vision_model.blocks.6.attn.proj.weight', 'clip_vision_model.head.weight', 'clip_vision_model.blocks.19.norm1.bias', 'clip_vision_model.blocks.17.mlp.w2.bias', 'pulid_encoder.layers.4.0.norm2.bias', 'clip_vision_model.blocks.16.attn.k_proj.weight', 'pulid_encoder.mapping_3.3.weight', 'clip_vision_model.blocks.3.mlp.w3.bias', 'clip_vision_model.blocks.8.attn.rope.freqs_sin', 'clip_vision_model.blocks.19.norm2.weight', 'clip_vision_model.blocks.0.attn.inner_attn_ln.weight', 'pulid_ca.1.to_q.weight', 'clip_vision_model.blocks.0.norm1.bias', 'pulid_encoder.layers.2.0.norm2.bias', 'clip_vision_model.blocks.5.attn.v_bias', 'clip_vision_model.norm.weight', 'pulid_ca.6.norm1.weight', 'clip_vision_model.blocks.16.norm1.weight', 'clip_vision_model.blocks.3.attn.inner_attn_ln.weight', 'clip_vision_model.blocks.1.norm1.weight', 'clip_vision_model.blocks.0.attn.proj.weight', 'clip_vision_model.blocks.0.mlp.ffn_ln.weight', 'pulid_ca.8.norm1.weight', 'clip_vision_model.blocks.19.mlp.ffn_ln.weight', 'pulid_encoder.layers.4.0.to_kv.weight', 'pulid_encoder.layers.0.1.0.weight', 'clip_vision_model.blocks.4.mlp.w2.bias', 'clip_vision_model.blocks.6.mlp.w3.bias', 'clip_vision_model.blocks.16.mlp.w1.bias', 'clip_vision_model.blocks.0.attn.proj.bias', 'pulid_ca.13.norm1.bias', 'clip_vision_model.blocks.15.attn.v_bias', 'clip_vision_model.blocks.14.mlp.w2.bias', 'pulid_ca.19.norm1.bias', 'pulid_encoder.layers.8.0.norm2.weight', 'clip_vision_model.blocks.23.norm2.weight', 'clip_vision_model.blocks.23.attn.k_proj.weight', 'pulid_encoder.layers.4.1.0.bias', 'pulid_ca.17.norm2.weight', 'clip_vision_model.blocks.20.mlp.ffn_ln.weight', 'pulid_ca.4.norm2.weight', 'clip_vision_model.blocks.2.attn.rope.freqs_sin', 'pulid_encoder.mapping_4.4.weight', 'clip_vision_model.blocks.14.mlp.ffn_ln.bias', 'pulid_ca.5.norm2.bias', 'clip_vision_model.blocks.2.attn.rope.freqs_cos', 'clip_vision_model.blocks.16.mlp.w1.weight', 'clip_vision_model.blocks.19.norm1.weight', 'pulid_encoder.layers.5.0.norm2.weight', 'pulid_encoder.mapping_2.3.bias', 'pulid_encoder.layers.4.1.3.weight', 'pulid_ca.16.norm1.bias', 'pulid_ca.13.norm1.weight', 'clip_vision_model.blocks.9.attn.proj.bias', 'clip_vision_model.blocks.5.attn.q_proj.weight', 'clip_vision_model.blocks.23.mlp.w3.bias', 'clip_vision_model.blocks.11.attn.v_proj.weight', 'clip_vision_model.blocks.15.attn.v_proj.weight', 'pulid_encoder.layers.9.1.0.weight', 'pulid_encoder.layers.1.1.3.weight', 'pulid_encoder.mapping_3.6.weight', 'clip_vision_model.blocks.10.mlp.w1.weight', 'clip_vision_model.blocks.15.mlp.w1.bias', 'clip_vision_model.blocks.2.mlp.w2.bias', 'pulid_encoder.mapping_0.1.bias', 'pulid_encoder.layers.2.1.0.weight', 'clip_vision_model.blocks.21.attn.q_proj.weight', 'clip_vision_model.blocks.21.norm2.weight', 'pulid_encoder.layers.5.0.to_q.weight', 'clip_vision_model.blocks.6.attn.rope.freqs_sin', 'clip_vision_model.blocks.22.norm2.bias', 'clip_vision_model.blocks.2.norm2.bias', 'pulid_encoder.layers.6.0.norm2.bias', 'clip_vision_model.blocks.10.attn.q_proj.weight', 'clip_vision_model.blocks.22.mlp.w3.bias', 'clip_vision_model.blocks.16.attn.rope.freqs_sin', 'pulid_ca.0.norm1.bias', 'clip_vision_model.blocks.14.mlp.w3.bias', 'clip_vision_model.blocks.17.norm1.weight', 'pulid_encoder.layers.8.0.to_out.weight', 'clip_vision_model.blocks.10.mlp.w3.weight', 'pulid_encoder.layers.7.1.1.weight', 'clip_vision_model.blocks.1.attn.rope.freqs_cos', 'clip_vision_model.blocks.19.mlp.ffn_ln.bias', 'clip_vision_model.blocks.22.mlp.w2.weight', 'pulid_ca.19.norm2.weight', 'clip_vision_model.blocks.21.mlp.w3.weight', 'pulid_encoder.id_embedding_mapping.3.weight', 'clip_vision_model.blocks.0.mlp.w2.bias', 'clip_vision_model.blocks.12.norm1.weight', 'pulid_ca.1.norm1.bias', 'clip_vision_model.blocks.8.mlp.w1.bias', 'pulid_encoder.layers.7.1.0.weight', 'clip_vision_model.blocks.23.attn.v_proj.weight', 'clip_vision_model.blocks.4.attn.proj.bias', 'pulid_encoder.layers.1.0.to_kv.weight', 'clip_vision_model.blocks.11.mlp.w1.bias', 'pulid_encoder.layers.4.1.0.weight', 'pulid_ca.7.norm2.bias', 'clip_vision_model.blocks.21.mlp.w2.weight', 'pulid_encoder.layers.4.0.to_out.weight', 'pulid_ca.8.norm2.weight', 'clip_vision_model.blocks.7.attn.proj.weight', 'clip_vision_model.blocks.7.norm2.weight', 'clip_vision_model.blocks.2.attn.inner_attn_ln.bias', 'pulid_encoder.layers.9.0.norm2.bias', 'clip_vision_model.blocks.13.attn.proj.weight', 'pulid_ca.9.norm1.weight', 'clip_vision_model.blocks.23.attn.q_proj.weight', 'pulid_ca.10.norm1.weight', 'clip_vision_model.blocks.1.mlp.w1.bias', 'clip_vision_model.blocks.2.attn.q_bias', 'clip_vision_model.blocks.8.mlp.w2.bias', 'clip_vision_model.blocks.22.attn.proj.bias', 'clip_vision_model.blocks.2.attn.q_proj.weight', 'clip_vision_model.blocks.4.mlp.ffn_ln.bias', 'clip_vision_model.blocks.1.attn.proj.weight', 'clip_vision_model.blocks.20.attn.k_proj.weight', 'clip_vision_model.blocks.4.attn.q_proj.weight', 'pulid_ca.17.to_kv.weight', 'pulid_encoder.id_embedding_mapping.6.weight', 'pulid_encoder.layers.6.1.3.weight', 'clip_vision_model.blocks.12.norm1.bias', 'clip_vision_model.blocks.19.attn.inner_attn_ln.weight', 'clip_vision_model.blocks.9.mlp.w3.weight', 'pulid_ca.10.to_q.weight', 'clip_vision_model.blocks.13.norm2.weight', 'pulid_ca.15.to_kv.weight', 'clip_vision_model.blocks.3.attn.v_bias', 'pulid_encoder.layers.8.0.norm1.weight', 'clip_vision_model.blocks.1.attn.inner_attn_ln.bias', 'clip_vision_model.blocks.14.attn.proj.weight', 'clip_vision_model.blocks.17.attn.rope.freqs_sin', 'clip_vision_model.blocks.8.norm2.bias', 'pulid_ca.14.norm2.bias', 'clip_vision_model.blocks.18.attn.q_bias', 'pulid_ca.6.to_q.weight', 'clip_vision_model.blocks.14.mlp.ffn_ln.weight', 'pulid_encoder.layers.5.1.1.weight', 'clip_vision_model.blocks.11.attn.q_bias', 'pulid_encoder.layers.7.1.0.bias', 'pulid_encoder.layers.2.0.norm1.bias', 'pulid_encoder.layers.9.0.norm1.weight', 'clip_vision_model.blocks.7.mlp.w3.weight', 'pulid_ca.0.to_q.weight', 'pulid_ca.11.norm1.bias', 'pulid_encoder.layers.5.1.3.weight', 'pulid_encoder.id_embedding_mapping.4.bias', 'pulid_ca.12.norm2.bias', 'clip_vision_model.blocks.15.mlp.ffn_ln.bias', 'clip_vision_model.blocks.21.mlp.w3.bias', 'pulid_encoder.layers.1.0.to_out.weight', 'clip_vision_model.blocks.21.mlp.w1.weight', 'clip_vision_model.blocks.15.norm2.bias', 'pulid_encoder.layers.5.0.norm1.bias', 'clip_vision_model.blocks.13.mlp.ffn_ln.bias', 'pulid_encoder.mapping_4.0.weight', 'clip_vision_model.blocks.5.attn.k_proj.weight', 'clip_vision_model.blocks.9.mlp.ffn_ln.weight', 'clip_vision_model.blocks.3.attn.proj.weight', 'pulid_ca.5.norm1.weight', 'clip_vision_model.blocks.0.norm1.weight', 'clip_vision_model.blocks.20.mlp.ffn_ln.bias', 'pulid_ca.1.to_out.weight', 'pulid_encoder.mapping_0.1.weight', 'pulid_encoder.layers.2.0.to_q.weight', 'clip_vision_model.blocks.5.mlp.w1.bias', 'pulid_encoder.mapping_2.3.weight', 'clip_vision_model.blocks.17.mlp.ffn_ln.weight', 'pulid_ca.17.norm1.bias', 'clip_vision_model.blocks.22.mlp.w3.weight', 'clip_vision_model.blocks.13.norm1.weight', 'clip_vision_model.blocks.6.norm1.weight', 'pulid_encoder.mapping_0.3.bias', 'clip_vision_model.blocks.20.mlp.w3.bias', 'clip_vision_model.blocks.8.attn.q_proj.weight', 'clip_vision_model.blocks.20.attn.proj.bias', 'pulid_ca.12.to_q.weight', 'pulid_encoder.layers.8.1.1.weight', 'clip_vision_model.blocks.11.norm2.bias', 'clip_vision_model.blocks.13.mlp.w1.bias', 'clip_vision_model.blocks.21.attn.rope.freqs_cos', 'pulid_encoder.layers.1.0.to_q.weight', 'clip_vision_model.blocks.21.attn.k_proj.weight', 'clip_vision_model.blocks.0.norm2.weight', 'pulid_encoder.id_embedding_mapping.1.weight', 'pulid_ca.11.to_q.weight', 'pulid_encoder.layers.0.0.norm2.bias', 'clip_vision_model.blocks.6.attn.v_bias', 'pulid_ca.11.norm2.weight', 'pulid_encoder.mapping_3.0.weight', 'pulid_encoder.layers.3.1.0.weight', 'clip_vision_model.blocks.21.attn.rope.freqs_sin', 'pulid_encoder.mapping_3.4.bias', 'clip_vision_model.blocks.5.attn.v_proj.weight', 'clip_vision_model.blocks.12.attn.inner_attn_ln.weight', 'pulid_encoder.layers.0.0.to_kv.weight', 'pulid_encoder.layers.9.1.3.weight', 'clip_vision_model.blocks.4.attn.inner_attn_ln.weight', 'clip_vision_model.blocks.6.mlp.w2.bias', 'pulid_ca.3.to_q.weight', 'clip_vision_model.blocks.21.attn.q_bias', 'pulid_ca.7.to_out.weight', 'clip_vision_model.blocks.17.attn.rope.freqs_cos', 'clip_vision_model.blocks.13.attn.v_proj.weight', 'pulid_encoder.mapping_0.6.weight', 'clip_vision_model.blocks.11.norm1.bias', 'pulid_ca.0.to_kv.weight', 'clip_vision_model.blocks.4.norm2.weight', 'pulid_encoder.layers.9.0.norm2.weight', 'clip_vision_model.blocks.22.attn.v_proj.weight', 'clip_vision_model.blocks.12.mlp.w2.bias', 'clip_vision_model.blocks.23.attn.q_bias', 'clip_vision_model.blocks.9.mlp.w2.weight', 'clip_vision_model.blocks.2.mlp.w2.weight', 'clip_vision_model.blocks.3.norm1.weight', 'clip_vision_model.blocks.11.norm1.weight', 'pulid_encoder.mapping_2.0.weight', 'clip_vision_model.blocks.0.attn.rope.freqs_cos', 'pulid_ca.13.to_q.weight', 'pulid_ca.1.norm1.weight', 'clip_vision_model.blocks.6.norm1.bias', 'clip_vision_model.blocks.2.attn.proj.bias', 'pulid_ca.0.to_out.weight', 'clip_vision_model.blocks.1.mlp.w3.weight', 'clip_vision_model.blocks.0.attn.inner_attn_ln.bias', 'clip_vision_model.blocks.15.mlp.w1.weight', 'clip_vision_model.blocks.5.norm1.bias', 'clip_vision_model.blocks.21.attn.inner_attn_ln.weight', 'clip_vision_model.blocks.16.attn.proj.bias', 'clip_vision_model.blocks.0.attn.v_proj.weight', 'clip_vision_model.blocks.19.attn.v_proj.weight', 'pulid_encoder.layers.1.0.norm2.weight', 'clip_vision_model.blocks.19.mlp.w3.bias', 'clip_vision_model.blocks.2.attn.inner_attn_ln.weight', 'pulid_encoder.layers.6.0.norm2.weight', 'clip_vision_model.blocks.20.attn.inner_attn_ln.weight', 'clip_vision_model.blocks.4.norm2.bias', 'clip_vision_model.blocks.19.attn.q_proj.weight', 'clip_vision_model.blocks.11.mlp.w3.weight', 'clip_vision_model.blocks.21.norm1.bias', 'pulid_encoder.mapping_4.0.bias', 'clip_vision_model.blocks.16.norm2.weight', 'clip_vision_model.blocks.21.attn.v_bias', 'clip_vision_model.blocks.8.attn.proj.weight', 'clip_vision_model.blocks.20.mlp.w2.bias', 'pulid_ca.3.to_kv.weight', 'clip_vision_model.head.bias', 'clip_vision_model.blocks.9.attn.rope.freqs_cos', 'pulid_ca.6.norm2.bias', 'pulid_ca.15.norm1.weight', 'clip_vision_model.blocks.18.attn.v_proj.weight', 'clip_vision_model.blocks.4.attn.rope.freqs_cos', 'clip_vision_model.blocks.12.attn.proj.weight', 'clip_vision_model.blocks.16.attn.q_bias', 'pulid_ca.16.to_q.weight', 'clip_vision_model.blocks.1.norm2.bias', 'clip_vision_model.blocks.23.norm2.bias', 'pulid_encoder.layers.3.1.1.weight', 'clip_vision_model.blocks.15.mlp.w3.weight', 'clip_vision_model.blocks.17.mlp.w2.weight', 'clip_vision_model.blocks.6.attn.rope.freqs_cos', 'clip_vision_model.blocks.1.attn.v_bias', 'pulid_encoder.layers.5.0.norm2.bias', 'clip_vision_model.blocks.1.attn.q_proj.weight', 'pulid_ca.9.norm2.bias', 'clip_vision_model.blocks.12.attn.rope.freqs_sin', 'clip_vision_model.blocks.13.norm2.bias', 'clip_vision_model.blocks.18.attn.inner_attn_ln.bias', 'clip_vision_model.blocks.0.attn.k_proj.weight', 'clip_vision_model.blocks.7.mlp.ffn_ln.weight', 'clip_vision_model.blocks.10.attn.rope.freqs_sin', 'clip_vision_model.blocks.2.mlp.w1.weight', 'pulid_encoder.layers.3.0.norm1.weight', 'clip_vision_model.blocks.3.norm2.bias', 'pulid_encoder.layers.4.0.norm2.weight', 'clip_vision_model.blocks.14.mlp.w1.weight', 'clip_vision_model.blocks.18.attn.rope.freqs_cos', 'clip_vision_model.blocks.14.attn.inner_attn_ln.bias', 'clip_vision_model.blocks.16.mlp.w3.bias', 'clip_vision_model.blocks.14.mlp.w3.weight', 'pulid_encoder.mapping_0.6.bias', 'pulid_encoder.layers.0.1.3.weight', 'clip_vision_model.blocks.3.mlp.w2.weight', 'clip_vision_model.blocks.22.norm1.bias', 'pulid_encoder.layers.6.0.to_q.weight', 'clip_vision_model.blocks.3.attn.rope.freqs_cos', 'clip_vision_model.blocks.7.attn.rope.freqs_cos', 'clip_vision_model.blocks.15.mlp.w2.weight', 'clip_vision_model.blocks.14.attn.inner_attn_ln.weight', 'pulid_ca.11.to_kv.weight', 'clip_vision_model.blocks.10.mlp.w2.bias', 'pulid_ca.2.norm2.bias', 'pulid_encoder.mapping_1.3.bias', 'clip_vision_model.blocks.7.mlp.w1.weight', 'clip_vision_model.blocks.8.norm1.weight', 'clip_vision_model.blocks.12.mlp.w2.weight', 'pulid_encoder.id_embedding_mapping.0.weight', 'clip_vision_model.blocks.12.norm2.bias', 'pulid_encoder.mapping_0.0.bias', 'clip_vision_model.blocks.5.norm2.weight', 'clip_vision_model.blocks.6.attn.inner_attn_ln.weight', 'clip_vision_model.blocks.8.mlp.w3.weight', 'clip_vision_model.blocks.14.attn.v_bias', 'clip_vision_model.norm.bias', 'clip_vision_model.blocks.5.attn.proj.weight', 'clip_vision_model.blocks.4.mlp.w1.bias', 'clip_vision_model.blocks.7.attn.inner_attn_ln.bias', 'pulid_encoder.id_embedding_mapping.0.bias', 'clip_vision_model.blocks.8.attn.v_proj.weight', 'pulid_ca.9.to_kv.weight', 'clip_vision_model.blocks.18.attn.proj.weight', 'pulid_encoder.layers.4.0.to_q.weight', 'pulid_ca.15.to_out.weight', 'clip_vision_model.blocks.10.attn.v_bias', 'pulid_encoder.layers.8.0.to_q.weight', 'clip_vision_model.blocks.7.attn.k_proj.weight', 'clip_vision_model.blocks.3.attn.proj.bias', 'pulid_encoder.layers.0.1.0.bias', 'clip_vision_model.blocks.2.mlp.w3.weight', 'pulid_encoder.mapping_0.4.bias', 'clip_vision_model.blocks.3.mlp.w2.bias', 'clip_vision_model.blocks.19.mlp.w1.bias', 'pulid_encoder.mapping_3.1.bias', 'pulid_ca.2.norm2.weight', 'clip_vision_model.blocks.17.attn.v_bias', 'pulid_ca.11.norm2.bias', 'pulid_ca.4.to_q.weight', 'clip_vision_model.blocks.3.mlp.ffn_ln.bias', 'clip_vision_model.blocks.22.attn.proj.weight', 'clip_vision_model.blocks.1.mlp.w2.bias', 'clip_vision_model.blocks.12.mlp.w3.bias', 'pulid_ca.19.norm2.bias', 'clip_vision_model.blocks.16.attn.proj.weight', 'clip_vision_model.blocks.15.attn.rope.freqs_cos', 'clip_vision_model.blocks.3.norm1.bias', 'pulid_ca.1.to_kv.weight', 'clip_vision_model.blocks.11.attn.k_proj.weight', 'clip_vision_model.blocks.10.attn.proj.weight', 'pulid_encoder.layers.9.0.norm1.bias', 'clip_vision_model.blocks.2.mlp.w3.bias', 'clip_vision_model.blocks.23.mlp.w2.bias', 'clip_vision_model.blocks.15.attn.rope.freqs_sin', 'pulid_encoder.layers.2.1.3.weight', 'clip_vision_model.blocks.5.mlp.w2.weight', 'pulid_ca.9.norm1.bias', 'pulid_ca.13.to_out.weight', 'clip_vision_model.blocks.21.mlp.ffn_ln.bias', 'pulid_encoder.layers.7.0.to_q.weight', 'pulid_encoder.layers.6.0.norm1.bias', 'clip_vision_model.blocks.21.attn.v_proj.weight', 'clip_vision_model.blocks.13.attn.inner_attn_ln.bias', 'clip_vision_model.blocks.14.attn.rope.freqs_sin', 'clip_vision_model.blocks.6.mlp.w1.weight', 'clip_vision_model.blocks.20.attn.inner_attn_ln.bias', 'pulid_encoder.mapping_4.1.bias', 'clip_vision_model.blocks.6.mlp.w1.bias', 'clip_vision_model.blocks.17.mlp.ffn_ln.bias', 'pulid_encoder.layers.7.0.norm1.bias', 'clip_vision_model.blocks.6.mlp.w3.weight', 'clip_vision_model.blocks.5.mlp.w3.bias', 'pulid_encoder.layers.1.0.norm2.bias', 'pulid_ca.6.norm1.bias', 'pulid_encoder.layers.8.0.norm2.bias', 'pulid_ca.4.to_out.weight', 'clip_vision_model.blocks.1.attn.rope.freqs_sin', 'pulid_encoder.layers.9.0.to_kv.weight', 'clip_vision_model.blocks.23.norm1.bias', 'clip_vision_model.blocks.11.attn.rope.freqs_sin', 'clip_vision_model.blocks.16.attn.inner_attn_ln.weight', 'clip_vision_model.blocks.3.mlp.w3.weight', 'clip_vision_model.blocks.0.norm2.bias', 'clip_vision_model.blocks.7.attn.q_bias', 'clip_vision_model.blocks.23.attn.proj.bias', 'clip_vision_model.blocks.9.attn.v_bias', 'pulid_encoder.layers.3.1.0.bias', 'clip_vision_model.blocks.16.mlp.w3.weight', 'clip_vision_model.blocks.20.norm1.weight', 'pulid_ca.10.norm2.bias', 'clip_vision_model.blocks.9.attn.inner_attn_ln.weight', 'pulid_ca.3.norm2.weight', 'clip_vision_model.blocks.15.norm1.weight', 'clip_vision_model.blocks.16.norm2.bias', 'pulid_ca.2.norm1.bias', 'clip_vision_model.blocks.15.attn.q_bias', 'clip_vision_model.blocks.9.norm2.bias', 'clip_vision_model.blocks.15.attn.proj.bias', 'pulid_ca.0.norm2.weight', 'clip_vision_model.blocks.15.mlp.w2.bias', 'clip_vision_model.blocks.20.attn.q_proj.weight', 'clip_vision_model.blocks.5.mlp.w1.weight', 'clip_vision_model.blocks.13.attn.v_bias', 'clip_vision_model.blocks.14.mlp.w1.bias', 'pulid_ca.16.norm2.weight', 'pulid_encoder.layers.3.0.to_q.weight', 'clip_vision_model.blocks.9.mlp.w1.weight', 'pulid_encoder.mapping_1.1.bias', 'clip_vision_model.blocks.3.attn.rope.freqs_sin', 'clip_vision_model.blocks.22.attn.inner_attn_ln.weight', 'clip_vision_model.blocks.12.mlp.ffn_ln.bias', 'clip_vision_model.blocks.9.attn.proj.weight', 'clip_vision_model.blocks.5.attn.inner_attn_ln.weight', 'pulid_ca.3.to_out.weight', 'clip_vision_model.blocks.5.attn.inner_attn_ln.bias', 'pulid_ca.2.to_kv.weight', 'pulid_ca.19.to_q.weight', 'clip_vision_model.blocks.9.norm2.weight', 'pulid_encoder.layers.2.0.norm2.weight', 'clip_vision_model.blocks.0.attn.q_bias', 'clip_vision_model.blocks.18.mlp.w2.weight', 'clip_vision_model.blocks.20.attn.rope.freqs_sin', 'clip_vision_model.blocks.7.mlp.ffn_ln.bias', 'clip_vision_model.blocks.16.attn.v_bias', 'pulid_ca.15.norm2.weight', 'clip_vision_model.blocks.11.attn.inner_attn_ln.weight', 'pulid_ca.14.to_kv.weight', 'pulid_ca.10.to_kv.weight', 'pulid_ca.11.to_out.weight', 'clip_vision_model.blocks.16.mlp.w2.weight', 'pulid_ca.10.to_out.weight', 'clip_vision_model.blocks.0.attn.v_bias', 'clip_vision_model.blocks.17.attn.v_proj.weight', 'pulid_ca.18.norm1.bias', 'clip_vision_model.blocks.8.attn.q_bias', 'pulid_ca.8.norm2.bias', 'clip_vision_model.blocks.12.mlp.ffn_ln.weight', 'pulid_encoder.layers.8.0.to_kv.weight', 'clip_vision_model.blocks.2.norm2.weight', 'clip_vision_model.blocks.1.mlp.w2.weight', 'clip_vision_model.blocks.0.attn.q_proj.weight', 'pulid_encoder.layers.9.0.to_out.weight', 'clip_vision_model.blocks.18.attn.v_bias', 'clip_vision_model.blocks.23.attn.v_bias', 'clip_vision_model.blocks.23.mlp.w2.weight', 'pulid_encoder.layers.8.0.norm1.bias', 'clip_vision_model.blocks.13.mlp.w2.weight', 'clip_vision_model.blocks.15.attn.inner_attn_ln.weight', 'pulid_ca.14.norm1.bias', 'clip_vision_model.blocks.13.attn.inner_attn_ln.weight', 'pulid_ca.16.norm1.weight', 'pulid_encoder.layers.1.0.norm1.weight', 'pulid_encoder.layers.2.0.to_out.weight', 'pulid_ca.3.norm1.bias', 'pulid_encoder.mapping_1.1.weight', 'clip_vision_model.blocks.16.attn.q_proj.weight', 'clip_vision_model.blocks.17.attn.inner_attn_ln.bias', 'clip_vision_model.blocks.20.mlp.w2.weight', 'clip_vision_model.blocks.1.norm1.bias', 'pulid_ca.2.to_q.weight', 'pulid_ca.16.to_kv.weight', 'clip_vision_model.blocks.6.attn.inner_attn_ln.bias', 'pulid_ca.9.to_q.weight', 'clip_vision_model.blocks.6.attn.q_bias', 'clip_vision_model.blocks.10.attn.v_proj.weight', 'pulid_encoder.mapping_0.4.weight', 'pulid_ca.17.norm1.weight', 'clip_vision_model.blocks.18.attn.q_proj.weight', 'clip_vision_model.blocks.9.mlp.ffn_ln.bias', 'clip_vision_model.blocks.4.attn.v_bias', 'clip_vision_model.blocks.6.mlp.ffn_ln.weight', 'clip_vision_model.blocks.13.mlp.w3.weight', 'pulid_encoder.layers.2.1.0.bias', 'clip_vision_model.blocks.14.norm2.weight', 'pulid_encoder.layers.9.1.1.weight', 'clip_vision_model.blocks.10.mlp.w1.bias', 'clip_vision_model.blocks.23.attn.inner_attn_ln.weight', 'pulid_encoder.mapping_3.0.bias', 'pulid_encoder.mapping_4.1.weight', 'pulid_encoder.layers.3.0.to_out.weight', 'pulid_encoder.mapping_0.3.weight', 'pulid_ca.0.norm1.weight', 'pulid_encoder.layers.2.0.norm1.weight', 'clip_vision_model.blocks.7.attn.rope.freqs_sin', 'clip_vision_model.blocks.16.mlp.ffn_ln.bias', 'clip_vision_model.blocks.12.attn.rope.freqs_cos', 'pulid_encoder.mapping_2.1.weight', 'clip_vision_model.blocks.8.attn.rope.freqs_cos', 'clip_vision_model.blocks.5.attn.proj.bias', 'clip_vision_model.blocks.15.attn.inner_attn_ln.bias', 'pulid_encoder.layers.0.0.norm1.bias', 'pulid_encoder.mapping_4.3.weight', 'clip_vision_model.blocks.1.attn.q_bias', 'pulid_encoder.layers.0.1.1.weight', 'clip_vision_model.blocks.14.norm2.bias', 'clip_vision_model.blocks.0.mlp.w1.weight', 'pulid_ca.6.to_out.weight', 'clip_vision_model.blocks.0.mlp.w3.bias', 'clip_vision_model.blocks.16.attn.inner_attn_ln.bias', 'pulid_ca.5.to_out.weight', 'pulid_ca.15.norm1.bias', 'clip_vision_model.blocks.7.norm1.weight', 'clip_vision_model.blocks.2.attn.proj.weight', 'clip_vision_model.blocks.19.attn.proj.weight', 'pulid_encoder.id_embedding_mapping.1.bias', 'pulid_encoder.layers.1.0.norm1.bias', 'clip_vision_model.blocks.22.mlp.ffn_ln.bias', 'clip_vision_model.blocks.21.mlp.w1.bias', 'pulid_ca.5.to_kv.weight', 'pulid_ca.9.norm2.weight', 'clip_vision_model.blocks.11.mlp.ffn_ln.weight', 'clip_vision_model.blocks.19.mlp.w2.weight', 'pulid_encoder.mapping_2.6.bias', 'clip_vision_model.blocks.12.attn.v_bias', 'clip_vision_model.blocks.16.norm1.bias', 'clip_vision_model.blocks.4.mlp.w3.weight', 'pulid_ca.3.norm2.bias', 'pulid_ca.14.to_q.weight', 'clip_vision_model.blocks.21.attn.proj.weight', 'clip_vision_model.blocks.17.attn.q_bias', 'clip_vision_model.blocks.18.attn.k_proj.weight', 'clip_vision_model.blocks.17.attn.inner_attn_ln.weight', 'pulid_encoder.id_embedding_mapping.6.bias', 'clip_vision_model.blocks.18.mlp.w3.bias', 'clip_vision_model.blocks.17.mlp.w3.bias', 'clip_vision_model.blocks.1.attn.v_proj.weight', 'clip_vision_model.blocks.19.attn.inner_attn_ln.bias', 'clip_vision_model.blocks.9.attn.v_proj.weight', 'pulid_ca.4.norm2.bias', 'clip_vision_model.blocks.15.mlp.ffn_ln.weight', 'clip_vision_model.blocks.2.attn.k_proj.weight', 'pulid_ca.13.norm2.bias', 'clip_vision_model.blocks.5.attn.rope.freqs_cos', 'clip_vision_model.blocks.20.mlp.w1.bias', 'clip_vision_model.blocks.1.attn.k_proj.weight', 'clip_vision_model.blocks.20.attn.rope.freqs_cos', 'clip_vision_model.blocks.0.mlp.w3.weight', 'pulid_encoder.latents', 'clip_vision_model.blocks.19.mlp.w1.weight', 'pulid_encoder.layers.1.1.1.weight', 'pulid_encoder.layers.6.1.1.weight', 'clip_vision_model.blocks.7.norm2.bias', 'clip_vision_model.blocks.18.norm1.bias', 'clip_vision_model.blocks.1.mlp.w3.bias', 'clip_vision_model.blocks.20.norm1.bias', 'pulid_ca.15.to_q.weight', 'clip_vision_model.blocks.6.norm2.bias', 'clip_vision_model.blocks.22.attn.rope.freqs_cos', 'pulid_ca.8.to_out.weight', 'clip_vision_model.blocks.5.norm2.bias', 'pulid_encoder.layers.0.0.to_q.weight', 'pulid_encoder.layers.9.0.to_q.weight', 'clip_vision_model.blocks.13.mlp.w1.weight', 'pulid_ca.18.norm2.bias', 'clip_vision_model.blocks.8.mlp.ffn_ln.weight', 'clip_vision_model.blocks.7.mlp.w2.weight', 'clip_vision_model.blocks.22.attn.inner_attn_ln.bias', 'pulid_ca.4.to_kv.weight', 'clip_vision_model.blocks.7.mlp.w3.bias', 'clip_vision_model.blocks.20.attn.v_bias', 'clip_vision_model.blocks.5.attn.rope.freqs_sin', 'clip_vision_model.blocks.10.mlp.ffn_ln.weight', 'clip_vision_model.blocks.8.attn.proj.bias', 'pulid_ca.13.to_kv.weight', 'clip_vision_model.blocks.12.mlp.w3.weight', 'clip_vision_model.cls_token', 'clip_vision_model.blocks.5.mlp.ffn_ln.weight', 'pulid_ca.4.norm1.weight', 'pulid_ca.18.norm1.weight', 'pulid_encoder.mapping_3.3.bias', 'clip_vision_model.blocks.6.attn.proj.bias', 'pulid_ca.6.norm2.weight', 'clip_vision_model.blocks.23.attn.rope.freqs_cos', 'clip_vision_model.blocks.3.attn.v_proj.weight', 'clip_vision_model.blocks.2.mlp.w1.bias', 'clip_vision_model.blocks.11.attn.inner_attn_ln.bias', 'clip_vision_model.blocks.19.attn.v_bias', 'clip_vision_model.blocks.12.attn.k_proj.weight', 'pulid_encoder.layers.3.0.norm2.bias', 'pulid_encoder.mapping_4.4.bias', 'clip_vision_model.blocks.23.mlp.w3.weight', 'clip_vision_model.blocks.2.attn.v_proj.weight', 'clip_vision_model.blocks.14.attn.rope.freqs_cos', 'clip_vision_model.blocks.8.mlp.w2.weight', 'clip_vision_model.blocks.10.mlp.w3.bias', 'clip_vision_model.blocks.15.attn.q_proj.weight', 'clip_vision_model.blocks.4.norm1.bias', 'clip_vision_model.blocks.13.attn.proj.bias', 'pulid_encoder.id_embedding_mapping.3.bias', 'pulid_ca.2.to_out.weight', 'clip_vision_model.blocks.14.attn.q_bias', 'pulid_ca.3.norm1.weight', 'clip_vision_model.blocks.4.attn.q_bias', 'clip_vision_model.blocks.17.norm1.bias', 'clip_vision_model.blocks.10.attn.inner_attn_ln.bias', 'clip_vision_model.blocks.18.mlp.ffn_ln.bias', 'clip_vision_model.blocks.13.norm1.bias', 'pulid_encoder.layers.5.0.to_kv.weight', 'clip_vision_model.blocks.23.mlp.ffn_ln.bias', 'clip_vision_model.blocks.4.mlp.ffn_ln.weight', 'clip_vision_model.blocks.17.mlp.w1.weight', 'pulid_encoder.layers.5.0.norm1.weight', 'clip_vision_model.blocks.3.mlp.w1.weight', 'clip_vision_model.blocks.0.mlp.w2.weight', 'clip_vision_model.blocks.9.mlp.w1.bias', 'pulid_ca.10.norm1.bias', 'pulid_encoder.layers.0.0.norm2.weight', 'pulid_ca.2.norm1.weight', 'clip_vision_model.blocks.4.attn.inner_attn_ln.bias', 'clip_vision_model.blocks.9.attn.q_proj.weight', 'clip_vision_model.blocks.10.mlp.ffn_ln.bias', 'clip_vision_model.blocks.11.attn.rope.freqs_cos', 'clip_vision_model.blocks.19.attn.proj.bias', 'clip_vision_model.blocks.18.norm1.weight', 'pulid_encoder.layers.0.0.norm1.weight', 'clip_vision_model.blocks.17.norm2.bias', 'clip_vision_model.blocks.19.norm2.bias', 'clip_vision_model.blocks.6.attn.k_proj.weight', 'clip_vision_model.blocks.23.mlp.w1.bias', 'clip_vision_model.blocks.11.mlp.w2.weight', 'pulid_ca.18.to_q.weight', 'pulid_encoder.layers.1.1.0.weight', 'pulid_ca.14.to_out.weight', 'clip_vision_model.blocks.10.norm2.weight', 'clip_vision_model.blocks.12.attn.q_proj.weight', 'pulid_encoder.layers.6.0.to_kv.weight', 'clip_vision_model.blocks.1.norm2.weight', 'clip_vision_model.blocks.11.mlp.w1.weight', 'pulid_encoder.layers.6.0.norm1.weight', 'clip_vision_model.blocks.13.attn.rope.freqs_sin', 'pulid_ca.7.norm2.weight', 'clip_vision_model.pos_embed', 'clip_vision_model.blocks.6.mlp.w2.weight', 'pulid_ca.8.to_kv.weight', 'clip_vision_model.blocks.8.mlp.ffn_ln.bias', 'pulid_encoder.mapping_3.1.weight', 'pulid_encoder.layers.1.1.0.bias', 'pulid_encoder.id_embedding_mapping.4.weight', 'pulid_ca.18.norm2.weight', 'clip_vision_model.blocks.4.mlp.w2.weight', 'clip_vision_model.blocks.1.attn.proj.bias', 'clip_vision_model.blocks.10.attn.rope.freqs_cos', 'clip_vision_model.blocks.14.norm1.bias', 'clip_vision_model.blocks.22.mlp.w1.weight', 'clip_vision_model.blocks.21.norm1.weight', 'clip_vision_model.blocks.6.attn.v_proj.weight', 'pulid_ca.5.norm2.weight', 'clip_vision_model.blocks.9.norm1.bias', 'pulid_encoder.layers.2.0.to_kv.weight', 'pulid_ca.18.to_kv.weight', 'pulid_ca.19.to_out.weight', 'clip_vision_model.blocks.15.attn.proj.weight', 'clip_vision_model.blocks.18.norm2.bias', 'clip_vision_model.blocks.7.mlp.w2.bias', 'clip_vision_model.blocks.22.mlp.w1.bias', 'clip_vision_model.blocks.18.mlp.ffn_ln.weight', 'pulid_ca.16.to_out.weight', 'clip_vision_model.blocks.10.attn.inner_attn_ln.weight', 'clip_vision_model.blocks.6.attn.q_proj.weight', 'pulid_encoder.mapping_2.6.weight', 'clip_vision_model.rope.freqs_cos', 'clip_vision_model.blocks.9.attn.q_bias', 'clip_vision_model.blocks.4.attn.v_proj.weight', 'clip_vision_model.patch_embed.proj.bias', 'pulid_encoder.layers.7.1.3.weight', 'clip_vision_model.blocks.0.attn.rope.freqs_sin', 'clip_vision_model.blocks.2.attn.v_bias', 'clip_vision_model.blocks.3.norm2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from transformer_flux_ca import FluxTransformer2DModelCA\n",
    "# from diffusers import FluxTransformer2DModel\n",
    "import torch\n",
    "\n",
    "weight_dtype = torch.bfloat16\n",
    "device = 'cuda'\n",
    "# transformer = FluxTransformer2DModel.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=weight_dtype, subfolder='transformer')\n",
    "# transformer = FluxTransformer2DModelCA.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=weight_dtype, subfolder='transformer', low_cpu_mem_usage=False, device_map=None)\n",
    "transformer = FluxTransformer2DModelCA.from_pretrained(\"black-forest-labs/FLUX.1-Krea-dev\", torch_dtype=weight_dtype, subfolder='transformer', low_cpu_mem_usage=False, device_map=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4155935e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59385c7d1fdd408b9b09701ab81d306f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13dbf3d51fc417d9fd0c14e77780fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    }
   ],
   "source": [
    "from pulid.utils import resize_numpy_image_long\n",
    "from flux.sampling import denoise, get_noise, get_schedule, prepare, unpack, prepare_img, prepare_txt\n",
    "from diffusers import FluxPipeline\n",
    "from pipeline_flux_ca import FluxPipelineCA\n",
    "\n",
    "device = 'cuda'\n",
    "flux = FluxPipelineCA.from_pretrained(\"black-forest-labs/FLUX.1-Krea-dev\", transformer=transformer, torch_dtype=weight_dtype).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0858893",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Image:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_image shape: (512, 512, 3)\n",
      "fail to detect face using insightface, extract embedding on align face\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64150c7561147db8b6f77e3baa390da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Image:   0%|          | 0/1 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 59\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m g_list:\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m seed \u001b[38;5;129;01min\u001b[39;00m seed_list:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m         img = \u001b[43mflux\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m            \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m            \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mid_embed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mid_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43muncond_id_embed\u001b[49m\u001b[43m=\u001b[49m\u001b[43muncond_id_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m         image = img.images[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img.images, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m img.images\n\u001b[32m     71\u001b[39m         \u001b[38;5;66;03m# image.save(f\"{output_dir}/{src_num}.png\")\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/faceswap_omini/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/data2/jiwon/OminiControl/pipeline_flux_ca.py:473\u001b[39m, in \u001b[36mFluxPipelineCA.__call__\u001b[39m\u001b[34m(self, prompt, prompt_2, negative_prompt, negative_prompt_2, true_cfg_scale, height, width, num_inference_steps, sigmas, guidance_scale, num_images_per_prompt, generator, latents, prompt_embeds, pooled_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, negative_ip_adapter_image, negative_ip_adapter_image_embeds, negative_prompt_embeds, negative_pooled_prompt_embeds, output_type, return_dict, joint_attention_kwargs, callback_on_step_end, callback_on_step_end_tensor_inputs, max_sequence_length, id_embed, uncond_id_embed, id_weight)\u001b[39m\n\u001b[32m    470\u001b[39m timestep = t.expand(latents.shape[\u001b[32m0\u001b[39m]).to(latents.dtype)\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transformer.cache_context(\u001b[33m\"\u001b[39m\u001b[33mcond\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     noise_pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m        \u001b[49m\u001b[43mguidance\u001b[49m\u001b[43m=\u001b[49m\u001b[43mguidance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpooled_projections\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpooled_prompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtxt_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimg_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlatent_image_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjoint_attention_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoint_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# ID Embedding\u001b[39;49;00m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mid_embed\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mid_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43mid_weight\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mid_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m    488\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_true_cfg:\n\u001b[32m    489\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m negative_image_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/faceswap_omini/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/faceswap_omini/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/data2/jiwon/OminiControl/transformer_flux_ca.py:777\u001b[39m, in \u001b[36mFluxTransformer2DModelCA.forward\u001b[39m\u001b[34m(self, hidden_states, encoder_hidden_states, pooled_projections, timestep, img_ids, txt_ids, guidance, joint_attention_kwargs, controlnet_block_samples, controlnet_single_block_samples, return_dict, controlnet_blocks_repeat, id_embed, id_weight)\u001b[39m\n\u001b[32m    774\u001b[39m         ca_idx += \u001b[32m1\u001b[39m\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m777\u001b[39m     encoder_hidden_states, hidden_states = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemb\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage_rotary_emb\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_rotary_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjoint_attention_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjoint_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    784\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m index_block % \u001b[38;5;28mself\u001b[39m.double_interval == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m id_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    785\u001b[39m         \u001b[38;5;66;03m# print(\"add id cross attention for block\", index_block, \"ca idx\", ca_idx)\u001b[39;00m\n\u001b[32m    786\u001b[39m         hidden_states = hidden_states + id_weight * \u001b[38;5;28mself\u001b[39m.pulid_ca[ca_idx](id_embed, hidden_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/faceswap_omini/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/faceswap_omini/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/data2/jiwon/OminiControl/diffusers/src/diffusers/models/transformers/transformer_flux.py:443\u001b[39m, in \u001b[36mFluxTransformerBlock.forward\u001b[39m\u001b[34m(self, hidden_states, encoder_hidden_states, temb, image_rotary_emb, joint_attention_kwargs)\u001b[39m\n\u001b[32m    440\u001b[39m joint_attention_kwargs = joint_attention_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m    442\u001b[39m \u001b[38;5;66;03m# Attention.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnorm_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnorm_encoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_rotary_emb\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_rotary_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mjoint_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(attention_outputs) == \u001b[32m2\u001b[39m:\n\u001b[32m    451\u001b[39m     attn_output, context_attn_output = attention_outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/faceswap_omini/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/faceswap_omini/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/data2/jiwon/OminiControl/diffusers/src/diffusers/models/transformers/transformer_flux.py:342\u001b[39m, in \u001b[36mFluxAttention.forward\u001b[39m\u001b[34m(self, hidden_states, encoder_hidden_states, attention_mask, image_rotary_emb, **kwargs)\u001b[39m\n\u001b[32m    338\u001b[39m     logger.warning(\n\u001b[32m    339\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mjoint_attention_kwargs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_kwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m are not expected by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.processor.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and will be ignored.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m     )\n\u001b[32m    341\u001b[39m kwargs = {k: w \u001b[38;5;28;01mfor\u001b[39;00m k, w \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m attn_parameters}\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_rotary_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/data2/jiwon/OminiControl/diffusers/src/diffusers/models/transformers/transformer_flux.py:105\u001b[39m, in \u001b[36mFluxAttnProcessor.__call__\u001b[39m\u001b[34m(self, attn, hidden_states, encoder_hidden_states, attention_mask, image_rotary_emb)\u001b[39m\n\u001b[32m    102\u001b[39m encoder_key = encoder_key.unflatten(-\u001b[32m1\u001b[39m, (attn.heads, -\u001b[32m1\u001b[39m))\n\u001b[32m    103\u001b[39m encoder_value = encoder_value.unflatten(-\u001b[32m1\u001b[39m, (attn.heads, -\u001b[32m1\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m encoder_query = \u001b[43mattn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnorm_added_q\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m encoder_key = attn.norm_added_k(encoder_key)\n\u001b[32m    108\u001b[39m query = torch.cat([encoder_query, query], dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/faceswap_omini/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/faceswap_omini/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/faceswap_omini/lib/python3.12/site-packages/torch/nn/modules/normalization.py:402\u001b[39m, in \u001b[36mRMSNorm.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    400\u001b[39m \u001b[33;03m    Runs forward pass.\u001b[39;00m\n\u001b[32m    401\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrms_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/faceswap_omini/lib/python3.12/site-packages/torch/nn/functional.py:2924\u001b[39m, in \u001b[36mrms_norm\u001b[39m\u001b[34m(input, normalized_shape, weight, eps)\u001b[39m\n\u001b[32m   2920\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight):\n\u001b[32m   2921\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m   2922\u001b[39m         rms_norm, (\u001b[38;5;28minput\u001b[39m, weight), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight=weight, eps=eps\n\u001b[32m   2923\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2924\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrms_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "flux.transformer.components_to_device(device)\n",
    "\n",
    "\n",
    "import cv2\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "output_dir = './results_krea_0.9.1/vgg_src'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "src_img_path_list = sorted(glob('/mnt/data2/dataset/vgg_data/src/*.jpg'))[:1]\n",
    "for src_img_path in tqdm(src_img_path_list, desc='Processing Image'):\n",
    "    prompt=\"a photo of human face\",\n",
    "    neg_prompt = \"\"\n",
    "    true_cfg = 1.0\n",
    "    use_true_cfg = True if true_cfg > 1.0 else False\n",
    "\n",
    "    src_num = os.path.basename(src_img_path).split('.')[0]\n",
    "    id_image = cv2.imread(src_img_path)\n",
    "    print(f\"id_image shape: {id_image.shape}\")\n",
    "    id_image = cv2.cvtColor(id_image, cv2.COLOR_BGR2RGB)\n",
    "    id_image = resize_numpy_image_long(id_image, 1024)\n",
    "    id_image_pil = Image.fromarray(id_image)\n",
    "    id_image_pil.save(f\"{output_dir}/{src_num}_id.png\")\n",
    "\n",
    "\n",
    "    # flux.t5, flux.clip = flux.t5.to(device), flux.clip.to(device)\n",
    "    # flux.pulid_model.components_to_device(device)\n",
    "    id_embeddings, uncond_id_embeddings = flux.transformer.get_id_embedding(id_image, cal_uncond=use_true_cfg)\n",
    "    id_embeddings = id_embeddings.to(device, dtype=weight_dtype)\n",
    "    if use_true_cfg:\n",
    "        uncond_id_embeddings = uncond_id_embeddings.to(device, dtype=weight_dtype)\n",
    "\n",
    "    # Nan check\n",
    "    if torch.isnan(id_embeddings).any():\n",
    "        raise RuntimeError('id embedding is nan')\n",
    "\n",
    "    # inp = prepare_txt(t5=flux.t5, clip=flux.clip, prompt=prompt, device=device)\n",
    "    # inp_neg = prepare_txt(t5=flux.t5, clip=flux.clip,  prompt=neg_prompt, device=device) if use_true_cfg else None\n",
    "    # flux.t5, flux.clip = flux.t5.cpu(), flux.clip.cpu()\n",
    "\n",
    "    from diffusers.utils import make_image_grid\n",
    "    # generate image\n",
    "    # prompt = \"a photo of human face\"\n",
    "    # negative_prompt = \"\"\n",
    "\n",
    "    # prompt = \"photo of a woman in red dress in a garden\"\n",
    "    # negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality, blurry\"\n",
    "    prompt = 'a photo of human face'\n",
    "    negative_prompt = ''\n",
    "\n",
    "    image_list = []\n",
    "\n",
    "    g_list = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "    seed_list = [0, 1, 2, 3, 4]\n",
    "    for g in g_list:\n",
    "        for seed in seed_list:\n",
    "            img = flux(\n",
    "                prompt=prompt,\n",
    "                negative_prompt=negative_prompt,\n",
    "                width=512,\n",
    "                height=512,\n",
    "                num_inference_steps=30,\n",
    "                guidance_scale=g,\n",
    "                generator=torch.Generator('cpu').manual_seed(seed),\n",
    "                id_embed=id_embeddings,\n",
    "                uncond_id_embed=uncond_id_embeddings,\n",
    "            )\n",
    "            image = img.images[0] if isinstance(img.images, list) else img.images\n",
    "            # image.save(f\"{output_dir}/{src_num}.png\")\n",
    "            image_list.append(image)\n",
    "\n",
    "    grid = make_image_grid(image_list, rows=len(g_list), cols=len(image_list)//len(g_list), resize=512)\n",
    "    grid.save(f\"{output_dir}/{src_num}.png\")\n",
    "\n",
    "\n",
    "\n",
    "    # g_list = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "    # for g in g_list:\n",
    "    #     for seed in [0, 1, 2, 3, 4]:\n",
    "    #         img = flux.generate_image_embed(\n",
    "    #             width=512,\n",
    "    #             height=512,\n",
    "    #             num_steps=30,\n",
    "    #             start_step=0,\n",
    "    #             guidance=g,\n",
    "    #             seed=seed,\n",
    "    #             id_embed=id_embeddings,\n",
    "    #             uncond_id_embed=uncond_id_embeddings,\n",
    "    #             inp=inp,\n",
    "    #             inp_neg=inp_neg,\n",
    "    #             prompt=prompt,\n",
    "    #             neg_prompt=\"\",\n",
    "    #             true_cfg=true_cfg,\n",
    "    #             timestep_to_start_cfg=1,\n",
    "    #             max_sequence_length=128,\n",
    "    #         )[0]\n",
    "    #         image = img[0] if isinstance(img, list) else img\n",
    "    #         image_list.append(image)\n",
    "\n",
    "    # grid = make_image_grid(image_list, rows=len(g_list), cols=len(image_list)//len(g_list), resize=512)\n",
    "    # grid.save(f\"{output_dir}/{src_num}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faceswap_omini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
